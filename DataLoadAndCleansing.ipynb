{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9d0be34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import nbimporter\n",
    "import Useful_Visualization_Functions\n",
    "from pyspark.ml import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark.sql.functions import col, explode, array, lit, lead, when, substring\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "387b62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.sql.functions import lit, col, column, expr, desc, asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8da508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install matplotlib\n",
    "# ! pip install seaborn\n",
    "# ! pip install ipynb\n",
    "# ! pip install nbimporter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eedc2b4-d9ad-4ec8-9582-efefa2ab9c8b",
   "metadata": {},
   "source": [
    "# Load and build data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a9b19-cd95-473d-861e-f259e1c80cf9",
   "metadata": {},
   "source": [
    "### Build the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e5c799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build our own SparkSession\n",
    "myspark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"AWS-Spark\")\\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\",6)\\\n",
    "    .config(\"spark.sql.repl.eagereval.enabled\",True)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42d2d3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-TL80VNL.Home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AWS-Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6bb7141190>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9ebda-5657-499a-be37-cade1e1b6b68",
   "metadata": {},
   "source": [
    "### Load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12e89599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25933550"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! head noaa.csv\n",
    "# noaa_data.show(10)\n",
    "#noaa_data = myspark.read.load(\"noaa.csv\", format=\"csv\", sep=\",\", header=True, inferSchema=True)\n",
    "noaa_csv_directory_path = \"/home/saltedcookie/Desktop/projetoABD/data\"\n",
    "\n",
    "noaa_csv_pathname = \"noaa.csv\"\n",
    "\n",
    "#df_raw = myspark.read.load(noaa_csv_directory_path, \n",
    "#                         format=\"csv\", \n",
    "#                         header=True, \n",
    "#                         pathGlobFilter=\"*.csv\",\n",
    "#                         recursiveFileLookup=True,\n",
    "#                         sep=\",\",\n",
    "#                         header=True,\n",
    "#                         inferSchema=True)\n",
    "\n",
    "df_raw = myspark.read.load(noaa_csv_pathname,\n",
    "                              format=\"csv\",\n",
    "                              sep=\",\",\n",
    "                              header=True,\n",
    "                              inferSchema=True)\n",
    "\n",
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d31b3512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- TEMP_ATTRIBUTES: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- DEWP_ATTRIBUTES: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- SLP_ATTRIBUTES: double (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- STP_ATTRIBUTES: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- VISIB_ATTRIBUTES: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- WDSP_ATTRIBUTES: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: double (nullable = true)\n",
      " |-- MAX_ATTRIBUTES: string (nullable = true)\n",
      " |-- MIN: double (nullable = true)\n",
      " |-- MIN_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHTT: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e4df6-d64d-4684-a45d-f9753659b07e",
   "metadata": {},
   "source": [
    "### Create columns \"ItRained\", \"NextDayIR\", \"ItRainedOrSnowed\" and \"NextDayIROS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69c93883-e0e9-45c1-9213-66e728fbc2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"ItRained\" is a column generated by using a part of the value of \"FRSHTT\"\n",
    "# \"NextDayIR\" is a column generated by using the value of the following row of \"ItRained\"\n",
    "# \"ItRainedOrSnowed\" is a column generated by using a part of the value of \"FRSHTT\"\n",
    "# \"NextDayIROS\" is a column generated by using the value of the following row of \"ItRainedOrSnowed\"\n",
    "\n",
    "df_new_columns = df_raw.withColumn(\"ItRained\", when((F.length(df_raw[\"FRSHTT\"]) <= 4), lit(0)) \\\n",
    "                    .when(F.length(df_raw[\"FRSHTT\"]) == 5, lit(1)) \\\n",
    "                    .otherwise(lit(substring('FRSHTT', 2, 1).cast(IntegerType()))) \\\n",
    ")\n",
    "df_new_columns = df_new_columns.withColumn( \"NextDayIR\", lead(\"ItRained\", default=2).over(Window.orderBy(\"STATION\")).alias(\"NextIR\") )\n",
    "\n",
    "df_new_columns = df_new_columns.withColumn(\"ItRainedOrSnowed\", when((F.length(df_raw[\"FRSHTT\"]) <= 3), lit(0)) \\\n",
    "                    .when((F.length(df_raw[\"FRSHTT\"]) == 4) | (F.length(df_raw[\"FRSHTT\"]) == 5), lit(1)) \\\n",
    "                    .otherwise(when((substring('FRSHTT', 2, 1) == \"1\") | (substring('FRSHTT', 3, 1) == \"1\"), lit(1)).otherwise(lit(0))) \\\n",
    ")\n",
    "df_new_columns = df_new_columns.withColumn( \"NextDayIROS\", lead(\"ItRainedOrSnowed\", default=2).over(Window.orderBy(\"STATION\")).alias(\"NextIROS\") )\n",
    "\n",
    "#df_raw.select(\"FRSHTT\", \"ItRained\", \"NextDayIR\", \"ItRainedOrSnowed\", \"NextDayIROS\").sample(False, 0.01).show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fa6f5",
   "metadata": {},
   "source": [
    "# Data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e8552-38d2-4696-a7cb-90da35df250a",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d54a464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_new_columns.columns\n",
    "\n",
    "#for cl in columns:\n",
    "#    noaa_data.describe(cl).show()\n",
    "#\n",
    "#for cl in columns:\n",
    "#    noaa_data.select(cl).distinct().show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a52fe125",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"STATION\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"NAME\", \"TEMP_ATTRIBUTES\", \"DEWP_ATTRIBUTES\",\n",
    "               \"SLP_ATTRIBUTES\", \"STP_ATTRIBUTES\", \"VISIB_ATTRIBUTES\", \"WDSP_ATTRIBUTES\", \"MAX_ATTRIBUTES\",\n",
    "               \"MIN_ATTRIBUTES\", \"PRCP_ATTRIBUTES\", \"GUST\", \"STP\"]\n",
    "\n",
    "cols_interest = [x for x in columns if x not in cols_to_drop]\n",
    "df_interest_cols = df_new_columns.select(cols_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d7f37-d964-432f-85e6-54087f12ce3b",
   "metadata": {},
   "source": [
    "### Drop nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a9de037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- MAX: double (nullable = true)\n",
      " |-- MIN: double (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHTT: integer (nullable = true)\n",
      " |-- ItRained: integer (nullable = true)\n",
      " |-- NextDayIR: integer (nullable = true)\n",
      " |-- ItRainedOrSnowed: integer (nullable = false)\n",
      " |-- NextDayIROS: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/25 11:03:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/25 11:03:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/25 11:04:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/25 11:04:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_interest_cols.printSchema()\n",
    "df_clean = df_interest_cols.dropna()\n",
    "[df_interest_cols.count(), df_clean.count()]\n",
    "\n",
    "columns = df_clean.columns\n",
    "\n",
    "#df_clean.select(\"SLP\").summary(\"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\", \"80%\", \"90%\").show()\n",
    "#df_clean.select(\"STP\").summary(\"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\", \"80%\", \"90%\").show()\n",
    "\n",
    "#for cl in df_clean.columns: \n",
    "#    print(cl)\n",
    "#    df_clean.select(cl).summary().show()\n",
    "#df_clean.select(\"ItRained\").summary().show()\n",
    "#\n",
    "#for cl in columns:\n",
    "#    df_clean.describe(cl).show()\n",
    "#\n",
    "#\n",
    "#for cl in columns:\n",
    "#    df_clean.select(cl).distinct().show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b410359-9fc8-43e2-85d8-17b629616dc2",
   "metadata": {},
   "source": [
    "### Drop error values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9a4aa2c-d3ec-4e61-9ce9-4852cb5eb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Antes: {df_clean.count()}\")\n",
    "\n",
    "df_clean = df_clean.filter(df_clean.TEMP > -10)\n",
    "df_clean = df_clean.filter(df_clean.DEWP < 100)\n",
    "df_clean = df_clean.filter(df_clean.SLP < 4000)\n",
    "#df_clean = df_clean.filter(df_clean.STP < 100)\n",
    "df_clean = df_clean.filter(df_clean.VISIB < 100)\n",
    "df_clean = df_clean.filter(df_clean.WDSP < 100)\n",
    "df_clean = df_clean.filter(df_clean.MXSPD < 100)\n",
    "# df_clean = df_clean.filter(df_clean.GUST < 100)\n",
    "df_clean = df_clean.filter(df_clean.MAX < 100)\n",
    "df_clean = df_clean.filter(df_clean.MIN < 100)\n",
    "df_clean = df_clean.filter(df_clean.PRCP < 100)\n",
    "df_clean = df_clean.filter(df_clean.SNDP < 100)\n",
    "\n",
    "#print(f\"Depois: {df_clean.count()}\")\n",
    "\n",
    "\n",
    "#temp_median = df_clean_pd['TEMP'].quantile(0.50)\n",
    "#df_clean_pd['TEMP'] = np.where(df_clean_pd['TEMP'] < -10, temp_median, df_clean_pd['TEMP'])\n",
    "#plt.boxplot(df_clean_pd[\"TEMP\"])\n",
    "#plt.show()\n",
    "#\n",
    "#dewp_median = df_clean_pd['DEWP'].quantile(0.50)\n",
    "#df_clean_pd['DEWP'] = np.where(df_clean_pd['DEWP'] > 100, dewp_median, df_clean_pd['DEWP'])\n",
    "#plt.boxplot(df_clean_pd[\"DEWP\"])\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a739708-4070-4853-92ca-8f20b1544101",
   "metadata": {},
   "source": [
    "# Save parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "989231f0-c7f5-44bd-a9e8-d6899af869e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/25 11:04:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/25 11:04:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/25 11:04:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/25 11:04:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/25 11:04:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/25 11:07:50 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-407b07b4-ef09-4c54-9b72-7f50753d88c5. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-407b07b4-ef09-4c54-9b72-7f50753d88c5\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1164)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:318)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:314)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:314)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:309)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1989)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:92)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$38(SparkContext.scala:667)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "cleanfilename = \"clean-noaa\"\n",
    "df_clean.write.mode(\"overwrite\").parquet(cleanfilename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
